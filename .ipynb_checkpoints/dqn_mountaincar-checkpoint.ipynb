{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import sample\n",
    "from statistics import mean, stdev\n",
    "import copy\n",
    "\n",
    "import gym\n",
    "\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay:\n",
    "    def __init__(self, maxlen=10000, batch_size=128):\n",
    "        self.maxlen = maxlen\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def add_exp(self, exp):\n",
    "        def buffer_full():\n",
    "            return True if len(self.buffer) == self.maxlen else False\n",
    "        \n",
    "        if buffer_full():\n",
    "            self.buffer.pop()\n",
    "        self.buffer.append(exp)\n",
    "        \n",
    "    def prime(self, env):\n",
    "        for exp in range(self.batch_size):\n",
    "            obs = env.reset()\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, _, _ = env.step(action)\n",
    "            next_action = env.action_space.sample()\n",
    "            \n",
    "            obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "            next_obs = torch.from_numpy(next_obs).float().unsqueeze(0)\n",
    "            \n",
    "            self.add_exp((obs, action, reward, next_obs, next_action))\n",
    "    \n",
    "    def sample(self):\n",
    "        return sample(self.buffer, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer(dim0, dim1):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim0, dim1),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(dims):\n",
    "    return [get_layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions, hidden_dim=128, n_hidden=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(*build_network([input_dim] + [hidden_dim]*n_hidden))\n",
    "        self.q_values = nn.Linear(hidden_dim, n_actions)\n",
    "        \n",
    "    def forward(self, X, action=None):\n",
    "        X = self.network(X)\n",
    "        q_values = self.q_values(X)\n",
    "        \n",
    "        if action:\n",
    "            q_values = q_values[torch.arange(len(action)), action].view(-1,1)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, episode, epsilon=0.3):\n",
    "    r = np.random.rand()\n",
    "    if episode > 50:\n",
    "        epsilon /= episode\n",
    "    return torch.argmax(q_values).item() if r > epsilon else np.random.randint(len(q_values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, q_network, optim, buffer, q_target, copy_network=100, n_episodes=500, max_steps=500, epsilon=0.3, gamma=0.9):\n",
    "    buffer.prime(env)\n",
    "    \n",
    "    returns = []\n",
    "    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        r = 0\n",
    "        \n",
    "        obs = env.reset()\n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "        \n",
    "        for step in range(1, max_steps+1):\n",
    "            q_values = q_network(obs)\n",
    "            action = epsilon_greedy(q_values, episode, epsilon=epsilon)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_obs = torch.from_numpy(next_obs).float().unsqueeze(0)\n",
    "            next_q_values = q_network(next_obs)\n",
    "            next_action = torch.argmax(next_q_values)\n",
    "            \n",
    "            buffer.add_exp((obs, action, reward, next_obs, next_action))\n",
    "            \n",
    "            r += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            \n",
    "            if done:\n",
    "                returns.append(r)\n",
    "                break\n",
    "                \n",
    "        batch = buffer.sample()\n",
    "        \n",
    "        obss, actions, rewards, next_obss, next_actions = [exp for exp in zip(*batch)]\n",
    "        \n",
    "        obss = torch.stack(obss).squeeze()\n",
    "        next_obss = torch.stack(next_obss).squeeze()\n",
    "        rewards = torch.tensor(rewards).view(-1,1)\n",
    "\n",
    "        td_loss = F.smooth_l1_loss(rewards + gamma*q_target(next_obss, next_actions), q_network(obss, actions))\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        td_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if episode % copy_network == 0:\n",
    "            q_target.load_state_dict(q_network.state_dict())\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print('Episode: {} - TD Loss: {:.4f} - Return: {} - Avg Return: {:.4f} - Std Return: {:.4f}'.format(episode, td_loss.item(), r, mean(returns), stdev(returns)))\n",
    "            \n",
    "    plt.plot(returns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(environment='MountainCar-v0', seed=0, n_hidden=2, hidden_dim=64, epsilon=1, gamma=0.9, buffer_size=10000, batch_size=64, copy_network=100):\n",
    "#     logger = Logger(output_dir='.', seed=seed)\n",
    "#     logger.save_config(locals())\n",
    "    \n",
    "    env = gym.make(environment)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    q_network = DQN(obs_dim, n_actions, hidden_dim=hidden_dim)\n",
    "    q_target = DQN(obs_dim, n_actions, hidden_dim=hidden_dim)\n",
    "    q_target.load_state_dict(q_network.state_dict())\n",
    "    q_target.eval()\n",
    "                             \n",
    "    optimizer = optim.Adam(q_network.parameters())\n",
    "    buffer = Replay()\n",
    "    \n",
    "    train(env, q_network, optimizer, buffer, q_target, copy_network=copy_network, epsilon=epsilon, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 - TD Loss: 0.3450 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 20 - TD Loss: 0.2244 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 30 - TD Loss: 0.1192 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 40 - TD Loss: 0.0303 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 50 - TD Loss: 0.0036 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 60 - TD Loss: 0.0053 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 70 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 80 - TD Loss: 0.0005 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 90 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 100 - TD Loss: 0.0005 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 110 - TD Loss: 0.1342 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 120 - TD Loss: 0.0116 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 130 - TD Loss: 0.0164 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 140 - TD Loss: 0.0021 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 150 - TD Loss: 0.0028 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 160 - TD Loss: 0.0008 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 170 - TD Loss: 0.0009 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 180 - TD Loss: 0.0005 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 190 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 200 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 210 - TD Loss: 0.0217 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 220 - TD Loss: 0.0440 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 230 - TD Loss: 0.0027 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 240 - TD Loss: 0.0050 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 250 - TD Loss: 0.0019 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 260 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 270 - TD Loss: 0.0009 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 280 - TD Loss: 0.0004 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 290 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 300 - TD Loss: 0.0007 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 310 - TD Loss: 0.0015 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 320 - TD Loss: 0.0230 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 330 - TD Loss: 0.0130 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 340 - TD Loss: 0.0009 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 350 - TD Loss: 0.0019 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 360 - TD Loss: 0.0020 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 370 - TD Loss: 0.0009 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 380 - TD Loss: 0.0008 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 390 - TD Loss: 0.0010 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 400 - TD Loss: 0.0008 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 410 - TD Loss: 0.0111 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 420 - TD Loss: 0.0045 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 430 - TD Loss: 0.0103 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 440 - TD Loss: 0.0045 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 450 - TD Loss: 0.0013 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 460 - TD Loss: 0.0010 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 470 - TD Loss: 0.0013 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 480 - TD Loss: 0.0011 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 490 - TD Loss: 0.0009 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n",
      "Episode: 500 - TD Loss: 0.0009 - Return: -200.0 - Avg Return: -200.0000 - Std Return: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
